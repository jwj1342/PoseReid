{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e878d6-62a1-4014-8dc0-8a58e8657278",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0  Loss:6.219603791381374 Time: 4.317269563674927 s\n",
      "Epoch:1  Loss:6.201304363481926 Time: 4.548346757888794 s\n",
      "Epoch:2  Loss:6.188093915130153 Time: 4.356888771057129 s\n",
      "Epoch:3  Loss:6.175030686638572 Time: 3.692657709121704 s\n",
      "Epoch:4  Loss:6.1634033737760605 Time: 3.608579397201538 s\n",
      "Epoch:5  Loss:6.154782288002245 Time: 3.4810919761657715 s\n",
      "Epoch:6  Loss:6.144701690384836 Time: 3.6089136600494385 s\n",
      "Epoch:7  Loss:6.137069947791822 Time: 3.6809165477752686 s\n",
      "Epoch:8  Loss:6.132002375342629 Time: 3.168250560760498 s\n",
      "Epoch:9  Loss:6.126477227066502 Time: 3.3992831707000732 s\n",
      "Epoch:10  Loss:6.121827934727524 Time: 3.4988927841186523 s\n",
      "Epoch:11  Loss:6.119309634873361 Time: 3.6661479473114014 s\n",
      "Epoch:12  Loss:6.115528865294023 Time: 3.473520517349243 s\n",
      "Epoch:13  Loss:6.113560091365468 Time: 4.186668395996094 s\n",
      "Epoch:14  Loss:6.110969875798081 Time: 4.150270462036133 s\n",
      "Epoch:15  Loss:6.107625282172001 Time: 3.197269916534424 s\n",
      "Epoch:16  Loss:6.104728207443699 Time: 3.2643420696258545 s\n",
      "Epoch:17  Loss:6.101654681292447 Time: 3.5523900985717773 s\n",
      "Epoch:18  Loss:6.097876859433724 Time: 3.406709671020508 s\n",
      "Epoch:19  Loss:6.09522582545425 Time: 3.599625825881958 s\n",
      "Epoch:20  Loss:6.090171777840816 Time: 3.4080405235290527 s\n",
      "Epoch:21  Loss:6.086783336870598 Time: 3.665729522705078 s\n",
      "Epoch:22  Loss:6.080844915274418 Time: 3.6057162284851074 s\n",
      "Epoch:23  Loss:6.074526533936009 Time: 3.323345422744751 s\n",
      "Epoch:24  Loss:6.067510944424254 Time: 3.4705986976623535 s\n",
      "Epoch:25  Loss:6.058947996659712 Time: 4.3540449142456055 s\n",
      "Epoch:26  Loss:6.047918500322284 Time: 4.438841819763184 s\n",
      "Epoch:27  Loss:6.0345924839828955 Time: 4.107801675796509 s\n",
      "Epoch:28  Loss:6.020526799288663 Time: 4.25776743888855 s\n",
      "Epoch:29  Loss:6.002994884144176 Time: 4.726640224456787 s\n",
      "Epoch:30  Loss:5.9840711463581435 Time: 4.480190753936768 s\n",
      "Epoch:31  Loss:5.96271673115817 Time: 4.221227407455444 s\n",
      "Epoch:32  Loss:5.941576134074818 Time: 4.635387420654297 s\n",
      "Epoch:33  Loss:5.919794573928371 Time: 4.277625560760498 s\n",
      "Epoch:34  Loss:5.900951501094934 Time: 4.870492935180664 s\n",
      "Epoch:35  Loss:5.882197394515529 Time: 4.874842882156372 s\n",
      "Epoch:36  Loss:5.862196683883667 Time: 4.802574396133423 s\n",
      "Epoch:37  Loss:5.846569964379976 Time: 4.581722259521484 s\n",
      "Epoch:38  Loss:5.82910696665446 Time: 4.952606916427612 s\n",
      "Epoch:39  Loss:5.811707951805809 Time: 4.832145929336548 s\n",
      "Epoch:40  Loss:5.797876437505086 Time: 4.657845973968506 s\n",
      "Epoch:41  Loss:5.783901286847664 Time: 4.575570106506348 s\n",
      "Epoch:42  Loss:5.771085406794692 Time: 4.66948390007019 s\n",
      "Epoch:43  Loss:5.756037069089485 Time: 4.004902601242065 s\n",
      "Epoch:44  Loss:5.744380141749526 Time: 4.41892671585083 s\n",
      "Epoch:45  Loss:5.732895237026793 Time: 4.726574420928955 s\n",
      "Epoch:46  Loss:5.7206667770038955 Time: 5.358933925628662 s\n",
      "Epoch:47  Loss:5.710302699695934 Time: 4.598679542541504 s\n",
      "Epoch:48  Loss:5.699213338620735 Time: 3.94283127784729 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from data_manager import VCM_Pose\n",
    "from dataset_preparation import PoseDataset_train, PoseDataset_test\n",
    "from net.net_moreinfo import PoseFeatureNet as net\n",
    "from util import IdentitySampler, GenIdx, load_model, save_model, test_general\n",
    "\n",
    "pose = VCM_Pose()\n",
    "\n",
    "rgb_pos, ir_pos = GenIdx(pose.rgb_label, pose.ir_label)\n",
    "\n",
    "num_pos = 2\n",
    "sampler = IdentitySampler(pose.ir_label, pose.rgb_label, rgb_pos, ir_pos, num_pos, 64)\n",
    "index1 = sampler.index1\n",
    "index2 = sampler.index2\n",
    "\n",
    "pose_dataset = PoseDataset_train(pose.train_ir, pose.train_rgb, seq_len=12, sample='video_train', transform=None,\n",
    "                                 index1=index1, index2=index2)\n",
    "\n",
    "dataloader = DataLoader(pose_dataset, batch_size=32*num_pos, num_workers=12, drop_last=True, sampler=sampler)\n",
    "\n",
    "criterion_CE = nn.CrossEntropyLoss()\n",
    "criterion_CE.to('cuda')\n",
    "\n",
    "net = net(500, 3, 128, 512, 1).cuda()\n",
    "\n",
    "nquery_1 = pose.num_query_tracklets_1\n",
    "ngall_1 = pose.num_gallery_tracklets_1\n",
    "nquery = pose.num_query_tracklets\n",
    "ngall = pose.num_gallery_tracklets\n",
    "\n",
    "queryloader = DataLoader(\n",
    "    PoseDataset_test(pose.query, seq_len=12, sample='video_test'),\n",
    "    batch_size=32, shuffle=False, num_workers=12)\n",
    "\n",
    "galleryloader = DataLoader(\n",
    "    PoseDataset_test(pose.gallery, seq_len=12, sample='video_test'),\n",
    "    batch_size=32, shuffle=False, num_workers=12)\n",
    "# ----------------visible to infrared----------------\n",
    "queryloader_1 = DataLoader(\n",
    "    PoseDataset_test(pose.query_1, seq_len=12, sample='video_test'),\n",
    "    batch_size=32, shuffle=False, num_workers=12)\n",
    "\n",
    "galleryloader_1 = DataLoader(\n",
    "    PoseDataset_test(pose.gallery_1, seq_len=12, sample='video_test'),\n",
    "    batch_size=32, shuffle=False, num_workers=12)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    config = {\n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"architecture\": \"LSTM&FC\",\n",
    "        \"dataset\": \"VCM-POSE\",\n",
    "        \"epochs\": 301,\n",
    "        \"momentum\":0.9,\n",
    "        \"name\":'testtest'\n",
    "    }\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(), \n",
    "        lr=config[\"learning_rate\"], \n",
    "        momentum=config[\"learning_rate\"], \n",
    "        weight_decay=5e-4)\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=config[\"learning_rate\"], weight_decay=5e-4)\n",
    "\n",
    "    best_mAP = 0.0\n",
    "    load_model(net, \"best_model_GCN_moreinfo1continue_75_0.039.pth\")\n",
    "    # 创建SummaryWriter\n",
    "    writer = SummaryWriter(\"/root/tf-logs/\"+config[\"name\"])\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()  # 开始时间\n",
    "        net.train()\n",
    "        for batch_idx, (imgs_ir, pids_ir, camid_ir, imgs_rgb, pids_rgb, camid_rgb) in enumerate(dataloader):\n",
    "            input1 = imgs_rgb\n",
    "            input2 = imgs_ir\n",
    "            label1 = pids_rgb\n",
    "            label2 = pids_ir\n",
    "\n",
    "            input_rgb = Variable(input1.float().to('cuda'))\n",
    "            input_ir = Variable(input2.float().to('cuda'))\n",
    "\n",
    "            labels = torch.cat((label1, label2), 0)  # 将两个模态的标签拼接起来，形成一个新的标签\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "            concat_feature, feature_cls = net(input_rgb, input_ir)\n",
    "\n",
    "            loss = criterion_CE(feature_cls, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        writer.add_scalar('Metrics/loss', avg_loss, epoch)\n",
    "        print(f\"Epoch:{epoch}  Loss:{avg_loss} Time: {end_time - start_time} s\")\n",
    "\n",
    "        # 下面书写保存模型的代码（要求在当两个模态的map任意一个达到新高的时候进行保存，保存时候使用相关信息命名）\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            net.eval()\n",
    "            cmc_t2v, mAP_t2v = test_general(galleryloader, queryloader, net, ngall, nquery)\n",
    "            writer.add_scalar('Metrics/mAP_t2v', mAP_t2v, epoch)\n",
    "            writer.add_scalar('Metrics/t2v-Rank-1', cmc_t2v[0], epoch)\n",
    "            writer.add_scalar('Metrics/t2v-Rank-20', cmc_t2v[4], epoch)\n",
    "\n",
    "            cmc_v2t, mAP_v2t = test_general(galleryloader_1, queryloader_1, net, ngall_1, nquery_1)\n",
    "            writer.add_scalar('Metrics/mAP_v2t', mAP_v2t, epoch)\n",
    "            writer.add_scalar('Metrics/v2t-Rank-1', cmc_v2t[0], epoch)\n",
    "            writer.add_scalar('Metrics/v2t-Rank-20', cmc_v2t[4], epoch)\n",
    "\n",
    "            if mAP_t2v + mAP_v2t > best_mAP:\n",
    "                best_mAP = mAP_t2v + mAP_v2t\n",
    "                save_model(net, f\"best_model_GCN_moreinfo22_{epoch}_{round(best_mAP, 3)}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
